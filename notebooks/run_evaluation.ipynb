{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run Evaluation\n",
        "\n",
        "\n",
        "Evaluates responses using MLflow and LLM-as-a-judge"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add src to path for imports (notebook runs from notebooks/ directory)\n",
        "notebook_dir = os.getcwd()\n",
        "repo_root = os.path.dirname(notebook_dir)\n",
        "src_path = os.path.join(repo_root, 'src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import uuid\n",
        "from verdict.evaluation.mlflow_evaluator import MLflowEvaluator\n",
        "from verdict.evaluation.custom_judges import LLMJudgeEvaluator\n",
        "from verdict.evaluation.deterministic_metrics import DeterministicMetricsCalculator\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Widget parameters\n",
        "dbutils.widgets.text(\"candidate_version\", \"\", \"Candidate Version\")\n",
        "dbutils.widgets.text(\"run_id\", \"\", \"Run ID (from inference)\")\n",
        "dbutils.widgets.text(\"judge_endpoint\", \"databricks-llama-4-maverick\", \"Judge Model Endpoint\")\n",
        "dbutils.widgets.text(\"catalog_name\", \"verdict\", \"Catalog Name\")\n",
        "\n",
        "candidate_version = dbutils.widgets.get(\"candidate_version\")\n",
        "run_id = dbutils.widgets.get(\"run_id\") or None\n",
        "judge_endpoint = dbutils.widgets.get(\"judge_endpoint\")\n",
        "catalog_name = dbutils.widgets.get(\"catalog_name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logger.info(f\"Starting evaluation for model version: {candidate_version}\")\n",
        "logger.info(f\"Judge endpoint: {judge_endpoint}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load responses from inference\n",
        "responses_table = f\"{catalog_name}.raw.model_responses\"\n",
        "responses_df = spark.table(responses_table)\n",
        "\n",
        "if run_id:\n",
        "    responses_df = responses_df.filter(f\"run_id = '{run_id}'\")\n",
        "\n",
        "# Join with prompts for ground truth\n",
        "prompts_df = spark.table(f\"{catalog_name}.raw.prompt_datasets\")\n",
        "responses_df = responses_df.join(\n",
        "    prompts_df.select(\"prompt_id\", \"prompt\", \"ground_truth\"),\n",
        "    on=\"prompt_id\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "response_count = responses_df.count()\n",
        "logger.info(f\"Loaded {response_count} responses for evaluation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run deterministic metrics\n",
        "logger.info(\"Computing deterministic metrics...\")\n",
        "det_calculator = DeterministicMetricsCalculator(catalog_name=catalog_name)\n",
        "metrics_df = det_calculator.calculate_metrics(responses_df)\n",
        "\n",
        "# Latency stats\n",
        "latency_stats = det_calculator.compute_latency_stats(metrics_df)\n",
        "logger.info(\"Latency statistics:\")\n",
        "latency_stats.display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run MLflow evaluation\n",
        "logger.info(\"Running MLflow LLM Evaluate...\")\n",
        "mlflow_evaluator = MLflowEvaluator(\n",
        "    catalog_name=catalog_name,\n",
        "    experiment_path=\"/verdict/experiments\"\n",
        ")\n",
        "\n",
        "eval_run_id = str(uuid.uuid4())\n",
        "mlflow_results = mlflow_evaluator.evaluate_responses(\n",
        "    responses_df=responses_df,\n",
        "    run_id=eval_run_id,\n",
        "    metrics=[\"faithfulness\", \"answer_relevance\", \"toxicity\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run LLM-as-a-judge evaluation\n",
        "logger.info(f\"Running LLM-as-a-judge evaluation with {judge_endpoint}...\")\n",
        "judge_evaluator = LLMJudgeEvaluator(\n",
        "    catalog_name=catalog_name,\n",
        "    judge_endpoint=judge_endpoint,\n",
        "    max_workers=10\n",
        ")\n",
        "\n",
        "judge_results = judge_evaluator.evaluate(\n",
        "    responses_df=responses_df,\n",
        "    run_id=eval_run_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\n",
        "print(f\"\\nEvaluation Run ID: {eval_run_id}\")\n",
        "print(f\"Model Version: {candidate_version}\")\n",
        "print(f\"Judge Endpoint: {judge_endpoint}\")\n",
        "\n",
        "# Display summary\n",
        "eval_table = f\"{catalog_name}.evaluated.eval_results\"\n",
        "spark.sql(f\"\"\"\n",
        "    SELECT metric_name,\n",
        "           COUNT(*) as count,\n",
        "           AVG(metric_value) as avg_value,\n",
        "           MIN(metric_value) as min_value,\n",
        "           MAX(metric_value) as max_value\n",
        "    FROM {eval_table}\n",
        "    WHERE run_id = '{eval_run_id}'\n",
        "    GROUP BY metric_name\n",
        "\"\"\").display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Return values for downstream tasks\n",
        "dbutils.jobs.taskValues.set(\"eval_run_id\", eval_run_id)"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+json": {
      "runAs": "OWNER",
      "dashboards": []
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}