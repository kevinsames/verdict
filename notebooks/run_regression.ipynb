{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run Regression Detection\n",
        "\n",
        "\n",
        "Detects regressions between candidate and baseline model versions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add src to path for imports (notebook runs from notebooks/ directory)\n",
        "notebook_dir = os.getcwd()\n",
        "repo_root = os.path.dirname(notebook_dir)\n",
        "src_path = os.path.join(repo_root, 'src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import json\n",
        "from verdict.regression.regression_detector import RegressionDetector, VerdictLabel\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Widget parameters\n",
        "dbutils.widgets.text(\"candidate_version\", \"\", \"Candidate Version\")\n",
        "dbutils.widgets.text(\"baseline_version\", \"\", \"Baseline Version\")\n",
        "dbutils.widgets.text(\"dataset_version\", \"v1\", \"Dataset Version\")\n",
        "dbutils.widgets.text(\"eval_run_id\", \"\", \"Evaluation Run ID\")\n",
        "dbutils.widgets.text(\"threshold\", \"5.0\", \"Regression Threshold %\")\n",
        "dbutils.widgets.text(\"p_value\", \"0.05\", \"P-value Threshold\")\n",
        "dbutils.widgets.text(\"catalog_name\", \"verdict\", \"Catalog Name\")\n",
        "\n",
        "candidate_version = dbutils.widgets.get(\"candidate_version\")\n",
        "baseline_version = dbutils.widgets.get(\"baseline_version\")\n",
        "dataset_version = dbutils.widgets.get(\"dataset_version\")\n",
        "eval_run_id = dbutils.widgets.get(\"eval_run_id\") or None\n",
        "threshold = float(dbutils.widgets.get(\"threshold\"))\n",
        "p_value_threshold = float(dbutils.widgets.get(\"p_value\"))\n",
        "catalog_name = dbutils.widgets.get(\"catalog_name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logger.info(f\"Comparing candidate {candidate_version} vs baseline {baseline_version}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize detector\n",
        "detector = RegressionDetector(\n",
        "    catalog_name=catalog_name,\n",
        "    threshold_pct=threshold,\n",
        "    p_value_threshold=p_value_threshold,\n",
        "    experiment_path=\"/verdict/experiments\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run regression detection\n",
        "report = detector.detect_regression(\n",
        "    candidate_version=candidate_version,\n",
        "    baseline_version=baseline_version,\n",
        "    run_id=eval_run_id,\n",
        "    dataset_version=dataset_version\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display verdict\n",
        "verdict = report[\"verdict\"]\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"VERDICT: {verdict}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for comp in report[\"comparisons\"]:\n",
        "    metric_name = comp.get(\"metric_name\", \"unknown\")\n",
        "    is_regression = comp.get(\"is_regression\", False)\n",
        "    candidate_mean = comp.get(\"candidate_mean\", \"N/A\")\n",
        "    baseline_mean = comp.get(\"baseline_mean\", \"N/A\")\n",
        "    pct_change = comp.get(\"pct_change\", 0)\n",
        "    p_val = comp.get(\"p_value\", 1)\n",
        "\n",
        "    status = \"\u26a0\ufe0f  REGRESSION\" if is_regression else \"\u2713  OK\"\n",
        "    print(f\"\\n{metric_name}: {status}\")\n",
        "    if candidate_mean is not None:\n",
        "        print(f\"  Candidate: {candidate_mean:.4f}\")\n",
        "    if baseline_mean is not None:\n",
        "        print(f\"  Baseline:  {baseline_mean:.4f}\")\n",
        "    print(f\"  Change:    {pct_change:+.2f}%\")\n",
        "    print(f\"  P-value:   {p_val:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Return values for downstream tasks\n",
        "dbutils.jobs.taskValues.set(\"verdict\", verdict)\n",
        "dbutils.jobs.taskValues.set(\"verdict_report\", json.dumps(report))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get verdict history\n",
        "history_df = detector.get_verdict_history(model_version=candidate_version, limit=10)\n",
        "print(\"Recent verdict history:\")\n",
        "history_df.display()"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+json": {
      "runAs": "OWNER",
      "dashboards": []
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}