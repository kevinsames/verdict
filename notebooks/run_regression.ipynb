{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run Regression Detection",
        "\n\nDetects regressions between candidate and baseline model versions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install /Workspace/Redict/dist/verdict-*.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging",
        "import json",
        "from verdict.regression.regression_detector import RegressionDetector, VerdictLabel",
        "",
        "logging.basicConfig(level=logging.INFO)",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Widget parameters",
        "dbutils.widgets.text(\"candidate_version\", \"\", \"Candidate Version\")",
        "dbutils.widgets.text(\"baseline_version\", \"\", \"Baseline Version\")",
        "dbutils.widgets.text(\"dataset_version\", \"v1\", \"Dataset Version\")",
        "dbutils.widgets.text(\"eval_run_id\", \"\", \"Evaluation Run ID\")",
        "dbutils.widgets.text(\"threshold\", \"5.0\", \"Regression Threshold %\")",
        "dbutils.widgets.text(\"p_value\", \"0.05\", \"P-value Threshold\")",
        "dbutils.widgets.text(\"catalog_name\", \"verdict\", \"Catalog Name\")",
        "",
        "candidate_version = dbutils.widgets.get(\"candidate_version\")",
        "baseline_version = dbutils.widgets.get(\"baseline_version\")",
        "dataset_version = dbutils.widgets.get(\"dataset_version\")",
        "eval_run_id = dbutils.widgets.get(\"eval_run_id\") or None",
        "threshold = float(dbutils.widgets.get(\"threshold\"))",
        "p_value_threshold = float(dbutils.widgets.get(\"p_value\"))",
        "catalog_name = dbutils.widgets.get(\"catalog_name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logger.info(f\"Comparing candidate {candidate_version} vs baseline {baseline_version}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize detector",
        "detector = RegressionDetector(",
        "    catalog_name=catalog_name,",
        "    threshold_pct=threshold,",
        "    p_value_threshold=p_value_threshold,",
        "    experiment_path=\"/verdict/experiments\"",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run regression detection",
        "report = detector.detect_regression(",
        "    candidate_version=candidate_version,",
        "    baseline_version=baseline_version,",
        "    run_id=eval_run_id,",
        "    dataset_version=dataset_version",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display verdict",
        "verdict = report[\"verdict\"]",
        "print(\"\\n\" + \"=\" * 60)",
        "print(f\"VERDICT: {verdict}\")",
        "print(\"=\" * 60)",
        "",
        "for comp in report[\"comparisons\"]:",
        "    metric_name = comp.get(\"metric_name\", \"unknown\")",
        "    is_regression = comp.get(\"is_regression\", False)",
        "    candidate_mean = comp.get(\"candidate_mean\", \"N/A\")",
        "    baseline_mean = comp.get(\"baseline_mean\", \"N/A\")",
        "    pct_change = comp.get(\"pct_change\", 0)",
        "    p_val = comp.get(\"p_value\", 1)",
        "",
        "    status = \"\u26a0\ufe0f  REGRESSION\" if is_regression else \"\u2713  OK\"",
        "    print(f\"\\n{metric_name}: {status}\")",
        "    if candidate_mean is not None:",
        "        print(f\"  Candidate: {candidate_mean:.4f}\")",
        "    if baseline_mean is not None:",
        "        print(f\"  Baseline:  {baseline_mean:.4f}\")",
        "    print(f\"  Change:    {pct_change:+.2f}%\")",
        "    print(f\"  P-value:   {p_val:.4f}\")",
        "",
        "print(\"\\n\" + \"=\" * 60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Return values for downstream tasks",
        "dbutils.jobs.taskValues.set(\"verdict\", verdict)",
        "dbutils.jobs.taskValues.set(\"verdict_report\", json.dumps(report))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get verdict history",
        "history_df = detector.get_verdict_history(model_version=candidate_version, limit=10)",
        "print(\"Recent verdict history:\")",
        "history_df.display()"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+json": {
      "runAs": "OWNER",
      "dashboards": []
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}