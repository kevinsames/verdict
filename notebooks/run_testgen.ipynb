{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run Testgen\n",
        "\n",
        "\n",
        "Generate RAG test dataset from Qdrant and load to Unity Catalog"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add src to path for imports (notebook runs from notebooks/ directory)\n",
        "notebook_dir = os.getcwd()\n",
        "repo_root = os.path.dirname(notebook_dir)\n",
        "src_path = os.path.join(repo_root, 'src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from verdict.testgen import Settings, TestDatasetGenerator\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Widget parameters\n",
        "dbutils.widgets.text(\"qdrant_collection\", \"documents\", \"Qdrant Collection\")\n",
        "dbutils.widgets.text(\"qdrant_url\", \"http://localhost:6333\", \"Qdrant URL\")\n",
        "dbutils.widgets.text(\"dataset_version\", \"v1\", \"Dataset Version\")\n",
        "dbutils.widgets.text(\"catalog_name\", \"verdict_dev\", \"Catalog Name\")\n",
        "dbutils.widgets.text(\"limit\", \"200\", \"Max Chunks\")\n",
        "dbutils.widgets.text(\"output_dir\", \"/Volumes/verdict_dev/raw/testgen_output\", \"Output Directory\")\n",
        "\n",
        "qdrant_collection = dbutils.widgets.get(\"qdrant_collection\")\n",
        "qdrant_url = dbutils.widgets.get(\"qdrant_url\")\n",
        "dataset_version = dbutils.widgets.get(\"dataset_version\")\n",
        "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
        "limit = int(dbutils.widgets.get(\"limit\") or \"200\")\n",
        "output_dir = dbutils.widgets.get(\"output_dir\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Required environment variables (set via cluster spark env vars or secrets)\n",
        "# - AZURE_OPENAI_ENDPOINT\n",
        "# - AZURE_OPENAI_API_KEY\n",
        "# - QDRANT_API_KEY (optional, for Qdrant Cloud)\n",
        "\n",
        "logger.info(f\"Generating test dataset from Qdrant collection: {qdrant_collection}\")\n",
        "logger.info(f\"Qdrant URL: {qdrant_url}\")\n",
        "logger.info(f\"Dataset version: {dataset_version}\")\n",
        "logger.info(f\"Catalog: {catalog_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate required environment variables\n",
        "required_vars = [\"AZURE_OPENAI_ENDPOINT\", \"AZURE_OPENAI_API_KEY\"]\n",
        "missing = [v for v in required_vars if not os.environ.get(v)]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required environment variables: {missing}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create settings\n",
        "settings = Settings(\n",
        "    qdrant_collection=qdrant_collection,\n",
        "    qdrant_url=qdrant_url,\n",
        "    qdrant_scroll_limit=limit,\n",
        "    output_dir=output_dir,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate dataset\n",
        "generator = TestDatasetGenerator(settings)\n",
        "result = generator.generate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load to Unity Catalog if Q&A pairs were generated\n",
        "qa_count = result.get(\"qa_pairs\", 0)\n",
        "if qa_count > 0:\n",
        "    logger.info(f\"Loading {qa_count} Q&A pairs to Unity Catalog...\")\n",
        "    loaded = generator.load_to_catalog(\n",
        "        qa_pairs=result[\"qa_pairs_data\"],\n",
        "        version=dataset_version,\n",
        "        catalog_name=catalog_name,\n",
        "    )\n",
        "    print(f\"\\nSuccessfully loaded {loaded} prompts to {catalog_name}.raw.prompt_datasets (version: {dataset_version})\")\n",
        "else:\n",
        "    print(\"\\nNo Q&A pairs generated. Check Qdrant collection and connectivity.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\n",
        "print(f\"\\n=== Testgen Summary ===\")\n",
        "print(f\"Chunks processed: {result.get('chunks', 0)}\")\n",
        "print(f\"Q&A pairs generated: {qa_count}\")\n",
        "print(f\"Skipped chunks: {result.get('skipped', 0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Return values for downstream tasks\n",
        "dbutils.jobs.taskValues.set(\"testgen_qa_count\", qa_count)\n",
        "dbutils.jobs.taskValues.set(\"testgen_version\", dataset_version)"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+json": {
      "runAs": "OWNER",
      "dashboards": []
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}