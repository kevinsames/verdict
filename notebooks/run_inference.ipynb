{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run Inference\n",
        "\n",
        "\n",
        "Runs parallel inference against Model Serving endpoints"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -e ./src\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%restart_python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import uuid\n",
        "from verdict.inference.inference_runner import InferenceRunner\n",
        "from verdict.data.prompt_dataset import PromptDatasetManager\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Widget parameters\n",
        "dbutils.widgets.text(\"model_endpoint\", \"your-model-endpoint\", \"Model Endpoint\")\n",
        "dbutils.widgets.text(\"dataset_version\", \"v1\", \"Dataset Version\")\n",
        "dbutils.widgets.text(\"candidate_version\", \"\", \"Candidate Version (auto-detected if empty)\")\n",
        "dbutils.widgets.text(\"catalog_name\", \"verdict\", \"Catalog Name\")\n",
        "\n",
        "model_endpoint = dbutils.widgets.get(\"model_endpoint\")\n",
        "dataset_version = dbutils.widgets.get(\"dataset_version\")\n",
        "candidate_version = dbutils.widgets.get(\"candidate_version\") or None\n",
        "catalog_name = dbutils.widgets.get(\"catalog_name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logger.info(f\"Starting inference for endpoint: {model_endpoint}\")\n",
        "logger.info(f\"Dataset version: {dataset_version}\")\n",
        "logger.info(f\"Catalog: {catalog_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize managers\n",
        "inference_runner = InferenceRunner(catalog_name=catalog_name)\n",
        "dataset_manager = PromptDatasetManager(catalog_name=catalog_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load prompt dataset\n",
        "prompts_df = dataset_manager.load_dataset(dataset_version)\n",
        "prompt_count = prompts_df.count()\n",
        "logger.info(f\"Loaded {prompt_count} prompts from dataset version {dataset_version}\")\n",
        "\n",
        "if prompt_count == 0:\n",
        "    raise ValueError(f\"No prompts found in dataset version {dataset_version}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run inference\n",
        "run_id = str(uuid.uuid4())\n",
        "results_df = inference_runner.run_inference(\n",
        "    endpoint_name=model_endpoint,\n",
        "    prompt_dataset=prompts_df,\n",
        "    model_version=candidate_version,\n",
        "    run_id=run_id,\n",
        "    batch_size=100,\n",
        "    max_workers=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "total = results_df.count()\n",
        "success = results_df.filter(\"status = 'success'\").count()\n",
        "failed = total - success\n",
        "\n",
        "logger.info(f\"Inference complete: {success}/{total} successful ({failed} failed)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results\n",
        "print(f\"\\nRun ID: {run_id}\")\n",
        "print(f\"Model Endpoint: {model_endpoint}\")\n",
        "print(f\"Success Rate: {success}/{total} ({100*success/total:.1f}%)\")\n",
        "\n",
        "results_df.select(\"prompt_id\", \"response\", \"latency_ms\", \"status\").display()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Return run_id for downstream tasks\n",
        "dbutils.jobs.taskValues.set(\"run_id\", run_id)\n",
        "dbutils.jobs.taskValues.set(\"model_endpoint\", model_endpoint)\n",
        "dbutils.jobs.taskValues.set(\"success_rate\", success/total if total > 0 else 0)"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+json": {
      "runAs": "OWNER",
      "dashboards": []
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}