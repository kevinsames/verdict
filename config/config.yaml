# Verdict Configuration
# Update these values for your Azure Databricks environment

# Unity Catalog
catalog:
  name: verdict
  schemas:
    raw: raw
    evaluated: evaluated
    metrics: metrics

# Azure Databricks Authentication
# Supports: azure_ad (recommended), managed_identity, pat
azure:
  # Authentication method: "azure_ad", "managed_identity", or "pat"
  auth_method: "azure_ad"
  # Azure AD app registration (for azure_ad method)
  tenant_id: "${AZURE_TENANT_ID}"
  client_id: "${AZURE_CLIENT_ID}"
  client_secret: "${AZURE_CLIENT_SECRET}"
  # Azure Key Vault for secrets (optional)
  key_vault_name: "${AZURE_KEY_VAULT_NAME}"
  # Workspace URL (e.g., https://adb-<workspace-id>.<random>.azuredatabricks.net)
  workspace_url: "${DATABRICKS_HOST}"

# Model Endpoints
endpoints:
  # Model endpoint to evaluate (update with your endpoint name)
  model: "databricks-gpt-oss-20b"
  # Judge model for LLM-as-a-judge evaluation
  judge: "databricks-llama-4-maverick"

# MLflow
mlflow:
  experiment_path: "/verdict/experiments"

# Regression Detection
regression:
  # Threshold for flagging regression (percentage drop)
  threshold_pct: 5.0
  # Statistical significance level
  p_value_threshold: 0.05
  # Metrics to track for regression
  metrics:
    - faithfulness
    - answer_relevance
    - judge_score
    - rouge_l
    - latency_p95

# Alerts
alerts:
  # Webhook URL for notifications (use dbutils.secrets for sensitive values)
  webhook_url_secret: "verdict/alerts_webhook"
  # Email recipients (comma-separated)
  email_recipients_secret: "verdict/alert_emails"
  # Alert on these verdict labels
  on_labels:
    - WARN
    - FAIL

# Evaluation
evaluation:
  # Sample size for evaluation (null = all)
  sample_size: null
  # Batch size for parallel evaluation
  batch_size: 100